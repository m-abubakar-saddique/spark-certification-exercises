{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO933HnJmL6Fy9uNuqJHu/4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/m-abubakar-saddique/spark-certification-exercises/blob/dev/Learning_Spark_Ch_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ":<h1><center>Learning Spark Chapter 4 Exercises</center></h1>"
      ],
      "metadata": {
        "id": "7OytnYmbMu_z"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dd6t0uFzuR4X"
      },
      "source": [
        "<a id='installing-spark'></a>\n",
        "### Installing Spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6apGVff5h4ca"
      },
      "source": [
        "Install Dependencies:\n",
        "\n",
        "\n",
        "1.   Java 8\n",
        "2.   Apache Spark with hadoop and\n",
        "3.   Findspark (used to locate the spark in the system)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tt7ZS1_wGgjn"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3x0ZRLxjMVr"
      },
      "source": [
        "Set Environment Variables:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdOOq4twHN1K"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ACYMwhgHTYz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea434b1a-7dc2-451a-b0df-b4a21fa98b49"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data  spark-3.1.1-bin-hadoop3.2\tspark-3.1.1-bin-hadoop3.2.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KR1zLBk1998Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "09f5da91-d685-4985-8deb-b38a7f600e71"
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # Property used to format output tables better\n",
        "spark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fc631d4b970>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://cb5e37f87fb7:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloding st-fire-call data"
      ],
      "metadata": {
        "id": "olou3eziJ0ra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/databricks/LearningSparkV2/master/databricks-datasets/learning-spark-v2/flights/departuredelays.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uS6LG2F2k4c5",
        "outputId": "dc1d57c7-a9dd-450e-b9db-c8424261a8b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-06-30 09:50:01--  https://raw.githubusercontent.com/databricks/LearningSparkV2/master/databricks-datasets/learning-spark-v2/flights/departuredelays.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 33396236 (32M) [text/plain]\n",
            "Saving to: ‘departuredelays.csv’\n",
            "\n",
            "departuredelays.csv 100%[===================>]  31.85M  99.5MB/s    in 0.3s    \n",
            "\n",
            "2023-06-30 09:50:02 (99.5 MB/s) - ‘departuredelays.csv’ saved [33396236/33396236]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading Input Data"
      ],
      "metadata": {
        "id": "VCy-gum3NIjg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_file = \"departuredelays.csv\"\n",
        "# Read and create a temporary view\n",
        "# Infer schema (note that for larger files you\n",
        "# may want to specify the schema)\n",
        "schema = \"`date` STRING, `delay` INT, `distance` INT, `origin` STRING, `destination` STRING\"\n",
        "\n",
        "df = (spark.read.format(\"csv\")\n",
        "  .schema(schema)\n",
        " .option(\"header\", \"true\")\n",
        " .load(csv_file))\n",
        "df.createOrReplaceTempView(\"us_delay_flights_tbl\")"
      ],
      "metadata": {
        "id": "P368Zy9QNLS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EQ1HLRmNM1N",
        "outputId": "3274f1fa-aa4a-4bea-d23a-ef2bc2266ede"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----+--------+------+-----------+\n",
            "|    date|delay|distance|origin|destination|\n",
            "+--------+-----+--------+------+-----------+\n",
            "|01011245|    6|     602|   ABE|        ATL|\n",
            "|01020600|   -8|     369|   ABE|        DTW|\n",
            "+--------+-----+--------+------+-----------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## find all flights whose distance is greater than 1,000 miles"
      ],
      "metadata": {
        "id": "AKaJCUNWNv-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"SELECT distance, origin, destination\n",
        "FROM us_delay_flights_tbl WHERE distance > 1000\n",
        "ORDER BY distance DESC\"\"\").show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3P3QcOkGNhpA",
        "outputId": "b833e62d-0000-415e-f0be-b859c914bf1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------+-----------+\n",
            "|distance|origin|destination|\n",
            "+--------+------+-----------+\n",
            "|    4330|   HNL|        JFK|\n",
            "|    4330|   HNL|        JFK|\n",
            "|    4330|   HNL|        JFK|\n",
            "|    4330|   HNL|        JFK|\n",
            "|    4330|   HNL|        JFK|\n",
            "|    4330|   HNL|        JFK|\n",
            "|    4330|   HNL|        JFK|\n",
            "|    4330|   HNL|        JFK|\n",
            "|    4330|   HNL|        JFK|\n",
            "|    4330|   HNL|        JFK|\n",
            "+--------+------+-----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# find all flights between SFO & ORD with at least a two-hour delay"
      ],
      "metadata": {
        "id": "dcnh59fiN-3W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"SELECT date, delay origin, destination\n",
        "FROM us_delay_flights_tbl WHERE origin = 'SFO' and destination = 'ORD' and delay > 120\n",
        "ORDER BY delay DESC\"\"\").show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLhSwKa7N1Y_",
        "outputId": "d1bf88b3-0d30-4d72-cf1a-7caf42882ada"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------+-----------+\n",
            "|    date|origin|destination|\n",
            "+--------+------+-----------+\n",
            "|02190925|  1638|        ORD|\n",
            "|01031755|   396|        ORD|\n",
            "|01022330|   326|        ORD|\n",
            "|01051205|   320|        ORD|\n",
            "|01190925|   297|        ORD|\n",
            "|02171115|   296|        ORD|\n",
            "|01071040|   279|        ORD|\n",
            "|01051550|   274|        ORD|\n",
            "|03120730|   266|        ORD|\n",
            "|01261104|   258|        ORD|\n",
            "+--------+------+-----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert Spark unix timestamp to spark timestamp\n",
        "Using custom functions and datetime functionalotues"
      ],
      "metadata": {
        "id": "12KB1SSTwrH6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "from datetime import datetime\n",
        "from pyspark.sql.functions import concat, lit, to_timestamp, udf\n",
        "\n",
        "\n",
        "def convert_date_string(date_string):\n",
        "    date_time_obj = datetime.strptime(date_string, '%Y%m%d%H%M')\n",
        "    return date_time_obj.strftime('%Y-%m-%d %I:%M %p')\n",
        "\n",
        "newdf = df.withColumn('datetime', concat(lit('2022'), df.date))\n",
        "\n",
        "UDF = udf(convert_date_string, StringType())\n",
        "newdf = newdf.withColumn(\"datetime\", UDF(\"datetime\"))\n",
        "newdf.withColumn('datetime', to_timestamp(newdf.datetime, 'yyyy-MM-dd hh:mm a'))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        },
        "id": "zrLOjvPePADW",
        "outputId": "d93b85d0-966a-49fb-edf6-7d766b8aa4c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "+--------+-----+--------+------+-----------+-------------------+\n",
              "|    date|delay|distance|origin|destination|           datetime|\n",
              "+--------+-----+--------+------+-----------+-------------------+\n",
              "|01011245|    6|     602|   ABE|        ATL|2022-01-01 12:45:00|\n",
              "|01020600|   -8|     369|   ABE|        DTW|2022-01-02 06:00:00|\n",
              "|01021245|   -2|     602|   ABE|        ATL|2022-01-02 12:45:00|\n",
              "|01020605|   -4|     602|   ABE|        ATL|2022-01-02 06:05:00|\n",
              "|01031245|   -4|     602|   ABE|        ATL|2022-01-03 12:45:00|\n",
              "|01030605|    0|     602|   ABE|        ATL|2022-01-03 06:05:00|\n",
              "|01041243|   10|     602|   ABE|        ATL|2022-01-04 12:43:00|\n",
              "|01040605|   28|     602|   ABE|        ATL|2022-01-04 06:05:00|\n",
              "|01051245|   88|     602|   ABE|        ATL|2022-01-05 12:45:00|\n",
              "|01050605|    9|     602|   ABE|        ATL|2022-01-05 06:05:00|\n",
              "|01061215|   -6|     602|   ABE|        ATL|2022-01-06 12:15:00|\n",
              "|01061725|   69|     602|   ABE|        ATL|2022-01-06 17:25:00|\n",
              "|01061230|    0|     369|   ABE|        DTW|2022-01-06 12:30:00|\n",
              "|01060625|   -3|     602|   ABE|        ATL|2022-01-06 06:25:00|\n",
              "|01070600|    0|     369|   ABE|        DTW|2022-01-07 06:00:00|\n",
              "|01071725|    0|     602|   ABE|        ATL|2022-01-07 17:25:00|\n",
              "|01071230|    0|     369|   ABE|        DTW|2022-01-07 12:30:00|\n",
              "|01070625|    0|     602|   ABE|        ATL|2022-01-07 06:25:00|\n",
              "|01071219|    0|     569|   ABE|        ORD|2022-01-07 12:19:00|\n",
              "|01080600|    0|     369|   ABE|        DTW|2022-01-08 06:00:00|\n",
              "+--------+-----+--------+------+-----------+-------------------+\n",
              "only showing top 20 rows"
            ],
            "text/html": [
              "<table border='1'>\n",
              "<tr><th>date</th><th>delay</th><th>distance</th><th>origin</th><th>destination</th><th>datetime</th></tr>\n",
              "<tr><td>01011245</td><td>6</td><td>602</td><td>ABE</td><td>ATL</td><td>2022-01-01 12:45:00</td></tr>\n",
              "<tr><td>01020600</td><td>-8</td><td>369</td><td>ABE</td><td>DTW</td><td>2022-01-02 06:00:00</td></tr>\n",
              "<tr><td>01021245</td><td>-2</td><td>602</td><td>ABE</td><td>ATL</td><td>2022-01-02 12:45:00</td></tr>\n",
              "<tr><td>01020605</td><td>-4</td><td>602</td><td>ABE</td><td>ATL</td><td>2022-01-02 06:05:00</td></tr>\n",
              "<tr><td>01031245</td><td>-4</td><td>602</td><td>ABE</td><td>ATL</td><td>2022-01-03 12:45:00</td></tr>\n",
              "<tr><td>01030605</td><td>0</td><td>602</td><td>ABE</td><td>ATL</td><td>2022-01-03 06:05:00</td></tr>\n",
              "<tr><td>01041243</td><td>10</td><td>602</td><td>ABE</td><td>ATL</td><td>2022-01-04 12:43:00</td></tr>\n",
              "<tr><td>01040605</td><td>28</td><td>602</td><td>ABE</td><td>ATL</td><td>2022-01-04 06:05:00</td></tr>\n",
              "<tr><td>01051245</td><td>88</td><td>602</td><td>ABE</td><td>ATL</td><td>2022-01-05 12:45:00</td></tr>\n",
              "<tr><td>01050605</td><td>9</td><td>602</td><td>ABE</td><td>ATL</td><td>2022-01-05 06:05:00</td></tr>\n",
              "<tr><td>01061215</td><td>-6</td><td>602</td><td>ABE</td><td>ATL</td><td>2022-01-06 12:15:00</td></tr>\n",
              "<tr><td>01061725</td><td>69</td><td>602</td><td>ABE</td><td>ATL</td><td>2022-01-06 17:25:00</td></tr>\n",
              "<tr><td>01061230</td><td>0</td><td>369</td><td>ABE</td><td>DTW</td><td>2022-01-06 12:30:00</td></tr>\n",
              "<tr><td>01060625</td><td>-3</td><td>602</td><td>ABE</td><td>ATL</td><td>2022-01-06 06:25:00</td></tr>\n",
              "<tr><td>01070600</td><td>0</td><td>369</td><td>ABE</td><td>DTW</td><td>2022-01-07 06:00:00</td></tr>\n",
              "<tr><td>01071725</td><td>0</td><td>602</td><td>ABE</td><td>ATL</td><td>2022-01-07 17:25:00</td></tr>\n",
              "<tr><td>01071230</td><td>0</td><td>369</td><td>ABE</td><td>DTW</td><td>2022-01-07 12:30:00</td></tr>\n",
              "<tr><td>01070625</td><td>0</td><td>602</td><td>ABE</td><td>ATL</td><td>2022-01-07 06:25:00</td></tr>\n",
              "<tr><td>01071219</td><td>0</td><td>569</td><td>ABE</td><td>ORD</td><td>2022-01-07 12:19:00</td></tr>\n",
              "<tr><td>01080600</td><td>0</td><td>369</td><td>ABE</td><td>DTW</td><td>2022-01-08 06:00:00</td></tr>\n",
              "</table>\n",
              "only showing top 20 rows\n"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E4MieJfvlD15"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}